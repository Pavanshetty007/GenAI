# ğŸ“š PDF Document Query Application 

*A Retrievalâ€‘Augmented Generation (RAG) system over multiple PDF documents, featuring BM25 + TFâ€‘IDF hybrid retrieval, Knowledge Graph lookup and timestamped chat history.*

---

## Table of Contents

1. [Features](#features)  
2. [Architecture & Approach](#architecture--approach)  
3. [Assumptions](#assumptions)  
4. [Prerequisites](#prerequisites)
5. [Project Structure](#project-structure)  
6. [Installation](#installation)  
7. [Running Ollama Locally (CPU)](#running-ollama-locally-cpu)  
8. [Usage](#usage)     
9. [Troubleshooting](#troubleshooting)  

---

## Features

- **Multiâ€‘PDF Upload & Indexing** via PDFPlumber & LangChain chunking  
- **Hybrid Retrieval** (BM25 sparse + TFâ€‘IDF dense) with tunable weights  
- **Knowledge Graph** using spaCy NER + RDFLib for factual lookups  
- **Substring Fallback** for exactâ€‘match formulas (e.g. positional encoding)    
- **Chat History** limited to lastÂ 5 exchanges, with Markdown timestamps  
- **Single Scrollbar UI** in Gradio Chatbot  

---

## Architecture & Approach

1. **Ingestion**  
   - PDFs are uploaded and copied into `static/`.  
   - Text is extracted pageâ€‘byâ€‘page, delimited by `\f`, then chunked with overlap.   

2. **Indexing**  
   - **Dense index**: TFâ€‘IDF vectorizer over chunks â†’ cosine similarity retrieval.  
   - **Sparse index**: Whoosh BM25 index â†’ keywordâ€‘based retrieval.  

3. **Hybrid Retrieval**  
   - Retrieve topâ€¯2Ã—TOP_K candidates from both methods.  
   - Score each chunk by weighted sum of sparse & dense ranks.  
   - Select topÂ TOP_K chunks for context.  

4. **Knowledge Graph (KG)**  
   - spaCy NER â†’ RDFLib triples `(entity, has_label, label)`.  
   - If a query token matches an entity, return KG result instead of RAG.  

5. **Fallback**  
   - Queries with â€œsin(â€, â€œPE(â€, etc. trigger substring fallback.    

6. **UI**  
   - Gradio chat interface with single scrollbar & Markdown timestamps.  

---

## Assumptions

- **Local Ollama** provides a CPUâ€‘based LLM (`gemma3`)â€”no GPU needed.  
- **PDFs** fit in memory; default chunk size =Â 500, overlap =Â 100.  
- **Environment**: Windows/Linux, PythonÂ 3.9+  

---

## Prerequisites

- PythonÂ 3.9+  
- [Ollama](https://ollama.com/) installed locally  
- No GPU required  
- miniconda installed
- Terminal/PowerShell access  

---

## project-structure
```
pdf_rag_app/
â”œâ”€â”€ run.py                    
â”œâ”€â”€ app.py                    
â”œâ”€â”€ config.py                 
â”œâ”€â”€ requirements.txt          
â”œâ”€â”€ static/                   
â”‚   â”œâ”€â”€ user.png              
â”‚   â””â”€â”€ bot.png               
â””â”€â”€ utils/                   
    â”œâ”€â”€ logger.py             
    â”œâ”€â”€ state.py              
    â”œâ”€â”€ pdf_utils.py          
    â”œâ”€â”€ sparse_retriever.py    
    â”œâ”€â”€ dense_retriever.py    
    â”œâ”€â”€ hybrid_retriever.py    
    â”œâ”€â”€ kg_utils.py            
    â”œâ”€â”€ keywords.py           
    â””â”€â”€ rag_utils.py          
```

## Ollama Installation
- Install Ollama per https://ollama.com/docs/install
- Use any model which has best accuracy (I have used opensource model gemma3 since i have hardware constraints)
```bash
ollama pull gemma3

ollama list
```
## Installation

```bash
git clone https://github.com/yourusername/adova-rag-chat.git
cd adova-rag-chat

# Create & activate a conda environment
conda create -n rag-chat python=3.10 -y
conda activate rag-chat

# Install dependencies
python.exe -m pip install --upgrade pip
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

```
## Usage
```bash
python run.py
```
- Open your browser at http://127.0.0.1:7860
- Upload PDFs in the left panel
- Click Process PDFs (repeat after adding new docs)
- Ask questions in the chat box & click Send
- The last 5 exchanges are retained by default for context.
- Use the "Clear Chat" button to reset the conversation thread.
## Troubleshooting
- Whoosh index errors:

```bash
rm -rf bm25_index
```
  then reâ€‘process PDFs.

- Ollama not running:

```bash
ollama list
# If empty, pull a model:
ollama pull gemma3
```

- spaCy model missing:

```bash
python -m spacy download en_core_web_sm
```